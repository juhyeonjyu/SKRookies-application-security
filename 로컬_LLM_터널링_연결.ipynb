{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_enoYBsKQLm",
        "outputId": "f1835061-35e6-4918-bf8d-88da0ba8db22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Ollama ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xMnPPlZTKfoR",
        "outputId": "80fb0dcc-0c50-4205-b558-e15d7f832ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama ì„œë²„ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...\n",
            "-> ì„œë²„ ì¤€ë¹„ ì™„ë£Œ.\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> Gradio ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2af03776aae5abf015.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2af03776aae5abf015.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloudflare í„°ë„ì„ ì‹¤í–‰í•˜ì—¬ ì™¸ë¶€ ì ‘ì† ì£¼ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\n",
            "\u001b[90m2025-08-08T01:15:03Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-08-08T01:15:03Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m |  https://grow-elite-flood-belle.trycloudflare.com                                          |\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.7.0 (Checksum 51e3909335fd7ba2ed5c696b0a6fb7d4a74f6a15bf36615cea0fccba620cfb3f)\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://127.0.0.1:7860]\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 3b5710bd-b429-40b6-9209-109bf94a8b73\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77\n",
            "2025/08/08 01:15:09 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m0dce3709-b991-40cb-8ddd-aa4b06853fce \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mlocation=\u001b[0mord06 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-08-08T01:16:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 13 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://127.0.0.1:7860\n",
            "\u001b[90m2025-08-08T01:16:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 13 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://grow-elite-flood-belle.trycloudflare.com/gradio_api/heartbeat/8xm744vbshq \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mtype=\u001b[0mhttp\n"
          ]
        }
      ],
      "source": [
        "# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
        "!pip install -q ollama gradio\n",
        "\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "import ollama\n",
        "\n",
        "# 2. Ollama ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n",
        "os.system(\"pkill ollama\")\n",
        "threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]), daemon=True).start()\n",
        "print(\"Ollama ì„œë²„ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...\")\n",
        "time.sleep(15) # ì„œë²„ê°€ ì¼œì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°\n",
        "print(\"-> ì„œë²„ ì¤€ë¹„ ì™„ë£Œ.\")\n",
        "\n",
        "# 3. ì‚¬ìš©í•  ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "!ollama pull deepseek-r1:1.5b\n",
        "\n",
        "# 4. ì±—ë´‡ ë‹µë³€ ìƒì„± í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "client = ollama.Client()\n",
        "\n",
        "# prompt(í˜„ì¬ ì…ë ¥), history(ê³¼ê±° ëŒ€í™” ê¸°ë¡), model(ëª¨ë¸ëª…)ì„ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
        "def generate_response(prompt, history, model):\n",
        "    try:\n",
        "        # Ollama APIê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ëŒ€í™” ê¸°ë¡ì„ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "        messages = []\n",
        "        for user_msg, assistant_msg in history:\n",
        "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "        # í˜„ì¬ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        # ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ëª¨ë¸ì—ê²Œ ë³´ë‚´ ë‹µë³€ì„ ìƒì„±í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
        "        response = client.chat(model=model, messages=messages)\n",
        "        return response['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"ì˜¤ë¥˜: {e}\"\n",
        "\n",
        "# 5. Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì„±í•˜ê³ , ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
        "chatbot_ui = gr.ChatInterface(\n",
        "    fn=generate_response,\n",
        "    title=\"ğŸ¤– DeepSeek ChatBot\",\n",
        "    additional_inputs=[gr.Dropdown(choices=[\"deepseek-r1:1.5b\"], value=\"deepseek-r1:1.5b\", label=\"ëª¨ë¸\")]\n",
        ")\n",
        "threading.Thread(target=lambda: chatbot_ui.launch(), daemon=True).start()\n",
        "print(\"-> Gradio ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "time.sleep(5) # Gradio ì„œë²„ê°€ ì¼œì§ˆ ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°\n",
        "\n",
        "# 6. Cloudflare í„°ë„ì„ ì‹¤í–‰í•˜ì—¬ ì™¸ë¶€ ì ‘ì† ì£¼ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "print(\"Cloudflare í„°ë„ì„ ì‹¤í–‰í•˜ì—¬ ì™¸ë¶€ ì ‘ì† ì£¼ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
        "!pkill cloudflared\n",
        "!wget -q -O cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared\n",
        "!./cloudflared tunnel --url http://127.0.0.1:7860"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}