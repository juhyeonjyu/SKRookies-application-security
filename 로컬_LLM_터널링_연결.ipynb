{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_enoYBsKQLm",
        "outputId": "f1835061-35e6-4918-bf8d-88da0ba8db22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Ollama 설치 스크립트를 다운로드하여 실행합니다.\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xMnPPlZTKfoR",
        "outputId": "80fb0dcc-0c50-4205-b558-e15d7f832ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama 서버를 준비 중입니다...\n",
            "-> 서버 준비 완료.\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> Gradio 챗봇 인터페이스가 백그라운드에서 실행되었습니다.\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2af03776aae5abf015.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2af03776aae5abf015.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloudflare 터널을 실행하여 외부 접속 주소를 생성합니다...\n",
            "\u001b[90m2025-08-08T01:15:03Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-08-08T01:15:03Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m |  https://grow-elite-flood-belle.trycloudflare.com                                          |\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.7.0 (Checksum 51e3909335fd7ba2ed5c696b0a6fb7d4a74f6a15bf36615cea0fccba620cfb3f)\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://127.0.0.1:7860]\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 3b5710bd-b429-40b6-9209-109bf94a8b73\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77\n",
            "2025/08/08 01:15:09 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-08-08T01:15:09Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m0dce3709-b991-40cb-8ddd-aa4b06853fce \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mlocation=\u001b[0mord06 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-08-08T01:16:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m  \u001b[31merror=\u001b[0m\u001b[31m\"stream 13 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m1 \u001b[36mingressRule=\u001b[0m0 \u001b[36moriginService=\u001b[0mhttp://127.0.0.1:7860\n",
            "\u001b[90m2025-08-08T01:16:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Request failed \u001b[31merror=\u001b[0m\u001b[31m\"stream 13 canceled by remote with error code 0\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mdest=\u001b[0mhttps://grow-elite-flood-belle.trycloudflare.com/gradio_api/heartbeat/8xm744vbshq \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mtype=\u001b[0mhttp\n"
          ]
        }
      ],
      "source": [
        "# 1. 필수 라이브러리를 설치합니다.\n",
        "!pip install -q ollama gradio\n",
        "\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "import ollama\n",
        "\n",
        "# 2. Ollama 서버를 시작합니다.\n",
        "os.system(\"pkill ollama\")\n",
        "threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]), daemon=True).start()\n",
        "print(\"Ollama 서버를 준비 중입니다...\")\n",
        "time.sleep(15) # 서버가 켜질 때까지 대기\n",
        "print(\"-> 서버 준비 완료.\")\n",
        "\n",
        "# 3. 사용할 모델을 다운로드합니다.\n",
        "!ollama pull deepseek-r1:1.5b\n",
        "\n",
        "# 4. 챗봇 답변 생성 함수를 정의합니다.\n",
        "client = ollama.Client()\n",
        "\n",
        "# prompt(현재 입력), history(과거 대화 기록), model(모델명)을 인자로 받습니다.\n",
        "def generate_response(prompt, history, model):\n",
        "    try:\n",
        "        # Ollama API가 요구하는 형식으로 대화 기록을 변환합니다.\n",
        "        messages = []\n",
        "        for user_msg, assistant_msg in history:\n",
        "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "        # 현재 사용자의 메시지를 대화 기록에 추가합니다.\n",
        "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "        # 전체 대화 기록을 모델에게 보내 답변을 생성하게 합니다.\n",
        "        response = client.chat(model=model, messages=messages)\n",
        "        return response['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"오류: {e}\"\n",
        "\n",
        "# 5. Gradio 인터페이스를 생성하고, 백그라운드 스레드에서 실행합니다.\n",
        "chatbot_ui = gr.ChatInterface(\n",
        "    fn=generate_response,\n",
        "    title=\"🤖 DeepSeek ChatBot\",\n",
        "    additional_inputs=[gr.Dropdown(choices=[\"deepseek-r1:1.5b\"], value=\"deepseek-r1:1.5b\", label=\"모델\")]\n",
        ")\n",
        "threading.Thread(target=lambda: chatbot_ui.launch(), daemon=True).start()\n",
        "print(\"-> Gradio 챗봇 인터페이스가 백그라운드에서 실행되었습니다.\")\n",
        "time.sleep(5) # Gradio 서버가 켜질 때까지 잠시 대기\n",
        "\n",
        "# 6. Cloudflare 터널을 실행하여 외부 접속 주소를 생성합니다.\n",
        "print(\"Cloudflare 터널을 실행하여 외부 접속 주소를 생성합니다...\")\n",
        "!pkill cloudflared\n",
        "!wget -q -O cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared\n",
        "!./cloudflared tunnel --url http://127.0.0.1:7860"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}